{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "#### First step in any data science or machine learning project\n",
    "\n",
    "make sure you have a folder named \"data\" in your project folder and the data folder is in the same folder as this     \n",
    "make sure you are in your virtural environment    \n",
    "\n",
    "python3 -m venv venv  \n",
    "source venv/bin/activate   # macOS/Linux  \n",
    "venv\\Scripts\\activate    # Windows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "print(\"Loading US Accidents dataset...\")\n",
    "file_path = 'data/US_Accidents_March23.csv'\n",
    "us_accidents = pd.read_csv(file_path)\n",
    "print(f\"Sample size: {len(us_accidents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset size\n",
    "print(us_accidents.shape)\n",
    "# Get column types \n",
    "print(us_accidents.info())\n",
    "# View first few rows\n",
    "print(us_accidents.head())\n",
    "# Check for missing values\n",
    "print(us_accidents.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Pratices\n",
    "# TODO: 1. Find the last 5 rows\n",
    "\n",
    "# TODO: 2. Find the mean value of \"Distance(mi)\"\n",
    "\n",
    "# TODO: 3. Find the number of unique values in \"Weather_Condition\"\n",
    "\n",
    "# TODO: 4. Find total number of accidents in MI\n",
    "\n",
    "# TODO: 5. Choose one column that interests you, find some patterns/interesting facts, and share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Sampling\n",
    "#### Sampling allows us to work with a smaller, representative subset while preserving key distributions\n",
    "\n",
    "Look into this link for more data sampling techniques  \n",
    "https://www.qualtrics.com/experience-management/research/sampling-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Goal: Extract a representative sample of 50,000 rows while maintaining the distribution of Severity.\n",
    "# Use simple random sampling with pandas.sample()\n",
    "us_accidents_sample = us_accidents.sample(n=50000, random_state=42) # random_state=42 ensures sample remains the same every time.\n",
    "print(us_accidents_sample[\"Severity\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the distribution of Severity of us_accidents\n",
    "# Is the distribution of Severity of us_accidents_sample similar to that of us_accidents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling may not preserve class proportions.\n",
    "# TODO: Try stratified sampling to preserve class proportions with sklearn \n",
    "# and compare the distribution with the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More practices: try other sampling methods on distribution of other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing\n",
    "#### Fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset\n",
    "Ways to handle missing values: https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/#h-list-of-methods-to-handle-missing-values-in-a-dataset  \n",
    "Handling missing values and outliers: https://medium.com/gen-ai-adventures/handling-missing-values-and-outliers-in-data-analysis-c1ffc2dd5051  \n",
    "Normalization methods: https://medium.com/@mkc940/different-normalization-methods-a1be71fe9f1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of null values for each column in descending order\n",
    "print(us_accidents_sample.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing End_Lat and End_Lng with Start_Lat and Start_Lng (if missing)\n",
    "us_accidents_sample[\"End_Lat\"].fillna(us_accidents_sample[\"Start_Lat\"], inplace=True)\n",
    "us_accidents_sample[\"End_Lng\"].fillna(us_accidents_sample[\"Start_Lng\"], inplace=True)\n",
    "# Drop irrelevant columns (e.g., ID if not useful for prediction).\n",
    "us_accidents_sample.drop(columns=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill missing values for \"Wind_Speed(mph)\" and \"Visibility(mi)\" with the mean of their respective columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Process all columns with missing values with appropriate methods (e.g., mean, median, mode, or drop etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Challenge: Apply KNN imputation for missing values of \"Start_Lat\" and \"Start_Lng\"\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "#### Preparing and cleaning the dataset to make it more suitable for machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on categorical column - \"Sunrise_Sunset\"\n",
    "print(us_accidents_sample[\"Sunrise_Sunset\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_accidents_sample[\"Sunrise_Sunset\"] = us_accidents_sample[\"Sunrise_Sunset\"].map({\"Day\": 0, \"Night\": 1})\n",
    "print(us_accidents_sample[\"Sunrise_Sunset\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose another categorical column and apply encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start_Time' to datetime and extract the year\n",
    "us_accidents_sample['Start_Time'] = us_accidents_sample['Start_Time'].str.split('.').str[0]\n",
    "us_accidents_sample['Start_Time'] = pd.to_datetime(us_accidents['Start_Time'].str.split('.').str[0], errors='coerce')\n",
    "us_accidents_sample['Year'] = us_accidents_sample['Start_Time'].dt.year\n",
    "us_accidents_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot of accidents by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "us_accidents_sample['Year'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Accidents by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does your plot show any interesting patterns or trends?\n",
    "# Does your sampled data show similar patterns/trends as the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. Extract hour of the day from \"Start_Time\"\n",
    "# TODO: 2. Create new feature: duration = (End_Time - Start_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3. Try Min-Max Normalization on \"Distance(mi)\" \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TODO: 4. Try Standardization on \"Precipitation(in)\"\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned sample to a new CSV file\n",
    "us_accidents_sample.to_csv(\"US_Accidents_Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT normalize/standardize categorical features!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for Traffic Flow Data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
